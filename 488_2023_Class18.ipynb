{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fvIg17w0I53"
      },
      "source": [
        "# **BUSI 488 / COMP 488 Data Science in the Business World**\n",
        "## *Spring 2023* \n",
        "Daniel M. Ringel  \n",
        "Kenan-Flagler Business School  \n",
        "*The University of North Carolina at Chapel Hill*  \n",
        "dmr@unc.edu\n",
        "\n",
        "## Customer Churn - Who to keep and who to let go?\n",
        "\n",
        "*March 21, 2023*  \n",
        "Version 2.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Call for Nominations: Recognize a Professor for their Teaching** \n",
        "\n",
        "> **Nominate here:** https://tinyurl.com/weatherspoon2023 \n",
        "\n",
        "![Weatherspoon](https://mapxp.app/BUSI488/Weatherspoon2023.png)\n",
        "\n",
        "\n",
        "# **Call for Nominations: Recognize a Professor for their Teaching** \n",
        "\n",
        " * The mediocre teacher tells. The good teacher explains. The superior teacher demonstrates. The great teacher inspires.  *William A. Ward*\n",
        "\n",
        " * I cannot teach anybody anything; I can only make them think. *Socrates*\n",
        "\n",
        " * Tell me and I forget. Teach me and I remember. Involve me and I learn. *Benjamin Franklin*\n",
        "\n",
        " > **Nominate here:** https://tinyurl.com/weatherspoon2023\n",
        "\n",
        "# ***Don't forget to put in YOUR nominations before Monday, March 27th, 2023!***"
      ],
      "metadata": {
        "id": "M0Nba3K1TpLL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKC_cmKb0Y3Y"
      },
      "source": [
        "# Today's Agenda\n",
        "> Discuss this notebook in your team (Team for TA3) for 40 minutes\n",
        "\n",
        "1. **What is Customer Churn**\n",
        "2. **Identify Customers that are at Risk of Churning**\n",
        "3. **Load and Clean Data**\n",
        "4. **EDA with Visualization**\n",
        "5. **Feature Engineering**\n",
        "6. **Data Preprocessing Pipeline**\n",
        "7. **Churn Prediction Model**\n",
        "8. **Making Things Better: Did we overlook something?**\n",
        "9. **Finalize Model**\n",
        "10. **Decide Who to Fight for and Who to let Go**\n",
        "11. **How well did we do?**\n",
        "12. **What Next?**\n",
        "\n",
        "> Discuss team findings in class\n",
        "\n",
        "## Prep-Check:\n",
        "- Reviewed notebook of class 17\n",
        "- Ran this entire notebook before class and reflected on the insights it creates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a7MgsbinsZ6"
      },
      "source": [
        "# 1. What is Customer Churn?\n",
        "\n",
        "\n",
        "![Lab vs Real-World](https://atrium.ai/wp-content/uploads/2021/07/What-stops-customer-churn-Having-a-centralized-data-hub-does-and-heres-why.jpeg)\n",
        "\n",
        "\n",
        "####***Customer churn***\n",
        "- customer attrition\n",
        "- customer turnover\n",
        "- customer defection\n",
        "\n",
        "***is the loss of clients or customers***\n",
        "\n",
        "Firms that have subscription or membership business models usually monitor customer churn closely:\n",
        "\n",
        "- Banks \n",
        "- Telephone service companies\n",
        "- Internet service providers\n",
        "- Pay TV companies\n",
        "- Insurance firms\n",
        "- Gyms\n",
        "- etc.   \n",
        "\n",
        "----------------\n",
        "\n",
        "####***Customer churn rates*** often a key business metric (along with cash flow, EBITDA (earnings before interest, tax, depreciation), etc.) \n",
        "* Cost of retaining an existing customer is far less than acquiring a new one.\n",
        "\n",
        "-----------------\n",
        "\n",
        "####Dedicated departments attempt to ***prevent churn*** and ***win back churned customers***   \n",
        "- long-term customers can be worth more than newly acquired customers \n",
        "\n",
        "####***BUT: Competitors*** may make special offers to entice customers away \n",
        "- Customers leave in hope of better service or value for money\n",
        "- ***Switching cost*** can create hurdles\n",
        "\n",
        "-----------------\n",
        "#### Important business activity: ***Customer Retention***\n",
        "- Can be costly -  *why?*\n",
        "- To focus retention efforts, must understand ***which customers are at risk of churning***.  \n",
        "\n",
        "  \n",
        "\n",
        "*Source: definition adapted from Wikipedia.com*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fjDo0Dd5lE_"
      },
      "source": [
        "# 2. **Today's Business Challenge:** How to Identify customers that are at risk of churning?\n",
        "- We will use a dataset that is based on real bank data, but was slightly modified for the purpose of this case study to \n",
        "    - preserve real customers privacies  \n",
        "    - preserve the bank's privacy  \n",
        "    - allow for richer analysis  \n",
        "\n",
        "###**The Bank's Problem:** Decide on retention measures for right customers.\n",
        "\n",
        "- What question(s) are we trying to answer?\n",
        "- How can the answer it/them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7EE5yNzw9Hf"
      },
      "source": [
        "# 3. Load and Clean Bank Data\n",
        "\n",
        "The bank provies us with two data sets: \n",
        "1. Data on bank customers that previously churned / did not churn (Training Set)\n",
        "2. Data on bank customers where the bank needs to decide on retention measures (New Customer Set)\n",
        "\n",
        "Both data sets contain the following variables: \n",
        "\n",
        "* ***ClientID:***  unique identifier of the bank customer\n",
        "* ***Surname:*** surname of customer\n",
        "* ***Firstname:*** firstname of customer\n",
        "* ***FICOScore:*** the average credit score of the customer in the past year\n",
        "* ***Subsidiary:*** the bank subsidiary that manages the customer relationship\n",
        "* ***Gender:*** Female or Male\n",
        "* ***Age:*** age of customer\n",
        "* ***Balance:*** total balance across all accounts (if applicable) such as checking, savings and credit\n",
        "* ***Product:*** number of banking products the customer uses\n",
        "* ***BankCC:*** whether the customer has a credit card from the bank\n",
        "* ***Active:*** indicates an active customer with regular transactions in the past 3 months\n",
        "* ***RegDeposits:*** average monthly deposits that are made to the account across the past year (e.g., salary or pension)\n",
        "* ***LifeInsur:*** whether the customer has a special life insurance policy from the bank\n",
        "* ***PlatStatus:*** whether the customer has Platinum status at the bank (receives several perks and better service)\n",
        "\n",
        "The training data set additionally includes the following variable:\n",
        "\n",
        "* ***Terminated:*** whether the customer closed their accounts with the bank within the 6 months following the the download of the data from the bank's database\n",
        "\n",
        "The new customer data set, on the other hand, additionally contains the following variable:\n",
        "\n",
        "* ***BnkRev:*** approximation of how much revenue the bank makes with each customer in a year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_oDOUfFMRzR"
      },
      "source": [
        "## 3.1 Import libraries\n",
        "Note: putting these at the top helps tell the users what libraries need to be installed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nc7raxbw9Hn"
      },
      "outputs": [],
      "source": [
        "# 1. Load what we will need for data wrangling, visualization, and modeling\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "pd.options.display.max_rows = None\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Support functions for much later modeling\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Classification Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Scoring Functions\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRRQ4gkGFN_s"
      },
      "outputs": [],
      "source": [
        "# 2. Add my Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# 3. Go to folder on Google Drive that contains files\n",
        "%cd /content/gdrive/MyDrive/MBA742/Class06\n",
        "\n",
        "# 4. Special shell command to view the files in the home directory of the notebook environment\n",
        "!ls "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K06LJeCM8Dlj"
      },
      "source": [
        "## 3.2 Load Training Data\n",
        "\n",
        "Let's load the data and describe it to get a fist feel for it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV5rw5ylw9Hr"
      },
      "outputs": [],
      "source": [
        "# 1. Read data file (training) into a pandas dataframe\n",
        "df = pd.read_json(\"Bank_Churn_Train.json\") # read in pandas Dataframe\n",
        "\n",
        "# 2. Number of rows (i.e., customer records) and columns (i.e., features)\n",
        "print(f\"\\n Number of Rows and Columns: {df.shape} \\n\")\n",
        "\n",
        "# 3. Take a look at the first 10 rows of the data\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW0jfYFeoIbR"
      },
      "source": [
        "## 3.3 Examine Data\n",
        "\n",
        "Let's take a first deeper look at our data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kMdNhqXdpVy"
      },
      "outputs": [],
      "source": [
        "# 1. Get a first impression\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSnmDP5VRmpX"
      },
      "source": [
        "#### Do you notice anything?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JTm4p0Zw9Hs"
      },
      "outputs": [],
      "source": [
        "# 2. What about data types? Do they make sense?\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV_uIYxzw9Hv"
      },
      "outputs": [],
      "source": [
        "# 3. Let's get unique counts for each variable\n",
        "df.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRXykE37SyOu"
      },
      "source": [
        "#### Does anything strike you as odd?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYrXGnKVSZuA"
      },
      "outputs": [],
      "source": [
        "# 3. Let's take a look at the values that the suspicious columns contain, for example, \"Active\"\n",
        "df['Active'].unique()\n",
        "\n",
        "# 3b. Others?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKBjwEFYw9Hu"
      },
      "outputs": [],
      "source": [
        "# 4. Check columns for missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mSYsDUzbM6R"
      },
      "source": [
        "#### When there are no missing values, we might still have to impute some values. **Why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiqdbGc9ucOL"
      },
      "source": [
        "## 3.4 Data Validity, Anomalies, and Missing Data\n",
        "\n",
        "We will handle numerical and categorical variables separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS2djT10shU-"
      },
      "source": [
        "### 3.4.1 Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpZjptloosQD"
      },
      "outputs": [],
      "source": [
        "# 1. Let's clean-up the categorical columns: Take a look first!\n",
        "\n",
        "# 1a. Define which columns are obviously categorical\n",
        "cat_cols = ['Gender','Subsidiary'] \n",
        "\n",
        "# 1b. Define which columns must be categorical because they have an indicator value (0,1)\n",
        "zero_one_cols = ['BankCC','Active','LifeInsur', 'PlatStatus','Terminated'] \n",
        "\n",
        "# 1c. Cycle through both types of categorical columns and print values and their frequencies\n",
        "for col in cat_cols+zero_one_cols:  \n",
        "  print(col)\n",
        "  print(df[col].value_counts( ))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "328FVm67ELY6"
      },
      "outputs": [],
      "source": [
        "# NOTE, that dtype in the output above does not refer to the variable, but to the counts that are output!\n",
        "df.PlatStatus.dtype # check data type of PlatStatus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssIiCAVjgH4l"
      },
      "source": [
        "#### **Let's clean up validity violations in the categorical data by:**\n",
        "- recoding them\n",
        "- enforcing boundaries\n",
        "\n",
        "***Let's create a dataframe cleaning function that can be used in a pipeline.***  \n",
        "- This function will directly modify the dataframe (not a copy)  \n",
        "  - Keeping this straight is important, because Pandas is very clever about not copying datasets that may be huge.\n",
        "  - Panda's *slice* concept: A dataframe may just be pointers to where the data is in a larger dataframe. \n",
        "  - You've probably seen the warning:\n",
        "      \n",
        "        SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. \n",
        "        Try using ```.loc[row_indexer,col_indexer] =``` value instead\n",
        "\n",
        "     This warns of potential confusion: are you asking to modify the original, large dataframe? Or to copy and modify just the data in the slice, while preserving the original?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STyNCC0rq4Oh"
      },
      "outputs": [],
      "source": [
        "# 3. Define a function that handles the cleaning of categorical variables\n",
        "''' \n",
        "This function is to clean categories in the Bank Churn data.\n",
        "Directly modifies the data frame df using .loc to avoid warning about 'copy of a slice'\n",
        "'''\n",
        "def clean_BankChurn_categories(df):\n",
        "  \n",
        "  # 3a. Identify the two types of categorical data \n",
        "  cat_cols = ['Gender','Subsidiary']\n",
        "  zero_one_cols = ['BankCC','Active','LifeInsur', 'PlatStatus','Terminated']\n",
        "\n",
        "  # 3b. Clean-up Gender \n",
        "  df.loc[df.Gender.str.startswith('F'), 'Gender'] = 'Female'  \n",
        "  df.loc[df.Gender.str.startswith('M'), 'Gender'] = 'Male'\n",
        "  \n",
        "  # 3c. Fix yes/no in zero_one_cols (PlatStatus needs this so others might, too.)\n",
        "  for col in zero_one_cols:\n",
        "    if (df[col].dtype == 'object'):  \n",
        "      df.loc[df[col] == 'yes', col] = '1' # recode \"yes\" to 1, if present \n",
        "      df.loc[df[col] == 'no', col] = '0' # recode \"no\" to 0, if present\n",
        "      df.loc[:,col] = df[col].astype(int) # make it really be 0-1\n",
        "\n",
        "  # 3d. Enforce boundaries for zero/one columns:\n",
        "  for col in zero_one_cols:\n",
        "    df.loc[:,col].clip(0,1, inplace = True) # \"clip\" assigns values outside boundary to boundary values.\n",
        "\n",
        "  # 3e. Typecast all categorical and zero/one columns to categorical\n",
        "  for col in cat_cols+zero_one_cols:\n",
        "    df.loc[:,col] = df[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwzF-4mrrYww"
      },
      "outputs": [],
      "source": [
        "# 4. Let's clean the categorical variables by calling our function!\n",
        "clean_BankChurn_categories(df)\n",
        "\n",
        "# 5. Check the types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw9xudT1iH3o"
      },
      "source": [
        "### 3.4.2 Numerical Variables\n",
        "\n",
        "Let's use some domain knowledge to identify customers where some of the data just don't make sense (i.e., are not valid)!  \n",
        "\n",
        "Ask yourself:\n",
        "- what is a valid FICO Score?\n",
        "- a valid Age?\n",
        "- a valid Account Balance?\n",
        "- plausible Regular Deposits?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86ErcivloPHl"
      },
      "outputs": [],
      "source": [
        "# Let's filter out all cases that are suspicious\n",
        "df.loc[(df.FICOScore < 300) | (df.Age > 100) | (df.Balance < -5000) | (df.Balance >5e5) | (df.Products > 10) | (df.RegDeposits < 0) | (df.RegDeposits > 1e5) ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyYFkKSzet0q"
      },
      "source": [
        "####**Let's clean up the numerical data by:**\n",
        "- imputing invalid (or implausible) values \n",
        "- dropping cases where implausibilities are not easily resolved through imputation\n",
        "- removing outliers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbLm629Kw0-g"
      },
      "outputs": [],
      "source": [
        "# 1. Define Outlier Detection Function (we will call this function within our numerical cleaning function that we define next)\n",
        "\n",
        "''' This function can be used on any dataset to return a list of index values for the outliers (based on standard deviation)\n",
        "Only appropriate for numerical features''' \n",
        "\n",
        "def get_outliers(data, columns):\n",
        "    # we create an empty list\n",
        "    outlier_idxs = []\n",
        "    # Number of standard deviations we keep. \n",
        "    nsd = 3 \n",
        "    for col in columns:\n",
        "        elements = data[col]\n",
        "        # we get the mean value for each column\n",
        "        mean = elements.mean()\n",
        "        # and the standard deviation of the column\n",
        "        sd = elements.std()\n",
        "        # we then get the index values of all values higher or lower than the mean +/- nsd standard deviations\n",
        "        outliers_mask = data[(data[col] > mean + nsd*sd) | (data[col]  < mean  - nsd*sd)].index\n",
        "        # and add those index values to our list\n",
        "        outlier_idxs  += [x for x in outliers_mask]\n",
        "    return list(set(outlier_idxs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPvmax6rwE0U"
      },
      "outputs": [],
      "source": [
        "# 2. Define a function that cleans up the anomalies in the numerical columns\n",
        "''' \n",
        "This function is to clean numeric fields of the Bank Churn data.\n",
        "Directly modifies the data frame df using .loc and drop inplace.\n",
        "'''\n",
        "def clean_BankChurn_numeric(df):\n",
        "  \n",
        "  # 2a. Impute invalid data with medians\n",
        "  df.loc[df.Age > 100,'Age'] = df.Age.median()\n",
        "  df.loc[df.Products > 10, 'Products'] = df.Products.median()\n",
        "  \n",
        "  # 2b. Mark rows with values outside of valid ranges by setting these values to None \n",
        "  df.loc[df.FICOScore<=0, 'FICOScore'] = None\n",
        "  df.loc[(df.Balance < -5000) | (df.Balance > 5e5), 'Balance'] = None\n",
        "  df.loc[(df.RegDeposits < 0) | (df.RegDeposits > 1e5), 'RegDeposits'] = None\n",
        "   \n",
        "  # 2c. Drop rows that contain missing values (and those set to None)\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  # 2d. Remove outliers for CERTAIN numerical variables only (using function defined above in 1.)\n",
        "  #numeric_features = df.select_dtypes(include=['int64', 'float64']).columns # this line would select all numerical values... why might that be a bad idea here?\n",
        "  numeric_features=['Balance','RegDeposits']\n",
        "  outs = get_outliers(df, numeric_features) # get indices of rows that contain outlier values\n",
        "  df.drop(outs, axis = 0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtV4CC6tidYu"
      },
      "outputs": [],
      "source": [
        "# 3. Let's use our function to clean-up the numeric columns\n",
        "clean_BankChurn_numeric(df)\n",
        "\n",
        "# 4. Check our work:\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHFI0Eadl1ip"
      },
      "source": [
        "## 3.5 Drop Unnecessary Columns\n",
        "Not all columns of a data set need (or should!) be included for the purpose of training a machine learning model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuDb_rdjmQHZ"
      },
      "outputs": [],
      "source": [
        "# 1. Take a look at the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw6N2oZYmf3U"
      },
      "outputs": [],
      "source": [
        "# 2. We won't need all of these variables - let's drop the ones that we think will not help our model\n",
        "df = df.drop([\"ClientID\", \"Surname\", \"Firstname\"], axis = 1) \n",
        "\n",
        "# 3. Check which ones are left\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcl9CQ5ew9Hw"
      },
      "source": [
        "**From the above, a couple of questions linger:**\n",
        "\n",
        "1. The data appears to be a snapshot as some point in time. E.g. the balance is for a given date, which leaves a lot of questions:\n",
        " - What date is it, and of what relevance is this date?\n",
        " - Would it be possible to obtain balances over a period of time as opposed to a single date?\n",
        "2. There are customers marked terminated that still have a balance in their account! What would this mean? Could they have terminated a product and not the bank?\n",
        "3. What does being an active member mean? Are there different degrees of activity? Might it be better to provide transaction counts, both for credits and debits?\n",
        "4. A breakdown of the products bought into by a customer could provide more information than the product count.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv1n_f5sw9Hx"
      },
      "source": [
        "# 4. Some EDA with Visualization\n",
        "\n",
        "Exploratory Data Analysis is useful to get a better feeling for the data and get a first glimpse at factors that might drive a certain phenomenon such as churn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZZOgvSNoRbY"
      },
      "source": [
        "## 4.1 Let's see how many customers actually churned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_FdWSXkw9Hx"
      },
      "outputs": [],
      "source": [
        "# 1. Create a pie chart\n",
        "labels = 'Churned', 'Retained'\n",
        "sizes = [df.Terminated[df['Terminated']==1].count(), df.Terminated[df['Terminated']==0].count()]\n",
        "explode = (0, 0.1)\n",
        "fig1, ax1 = plt.subplots(figsize=(10, 8))\n",
        "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90,textprops={'fontsize': 18})\n",
        "ax1.axis('equal')\n",
        "plt.title(\"Proportion of customers who exited (churn)\", size = 20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSm9urZiw9Hy"
      },
      "source": [
        "1. So, about 24% of the customers have churned. So the baseline model could be to predict that 24% of the customers will churn.  \n",
        "\n",
        "2. Given that 24% is a small number, we want to ensure that our chosen model can predict with greater accuracy than 24%\n",
        "\n",
        "- **Why?**\n",
        "\n",
        "3. Note, the bank wants to identify and retain customers that would churn (all of them?). The bank may be willing to trade for some inaccuracy on predicting the customers that are not going to churn.\n",
        "\n",
        "- **How would this impact your model?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6QpdixLosbf"
      },
      "source": [
        "## 4.2 Let's see whether some categorical variables are related to churn..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvNCixtZw9Hy"
      },
      "outputs": [],
      "source": [
        "# 1. Create Barcharts for key variables that are split by our target variable(Terminated)\n",
        "fig, axarr = plt.subplots(3, 2, figsize=(20, 18))\n",
        "sns.set(font_scale = 1.25)\n",
        "sns.countplot(x='Subsidiary', hue = 'Terminated',data = df, ax=axarr[0][0])\n",
        "sns.countplot(x='Gender', hue = 'Terminated',data = df, ax=axarr[0][1])\n",
        "sns.countplot(x='BankCC', hue = 'Terminated',data = df, ax=axarr[1][0])\n",
        "sns.countplot(x='Active', hue = 'Terminated',data = df, ax=axarr[1][1])\n",
        "sns.countplot(x='LifeInsur', hue = 'Terminated',data = df, ax=axarr[2][0])\n",
        "sns.countplot(x='PlatStatus', hue = 'Terminated',data = df, ax=axarr[2][1])\n",
        "fig.tight_layout(pad=2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Ewh54Zw9Hz"
      },
      "source": [
        "**We note the following:**\n",
        "\n",
        "- The majority of the data is from Boston customers. However, the proportion of churned customers is inversely related to the population of customers, suggesting that the bank may have a problem (maybe not enough customer service resources allocated) in the areas where it has fewer clients.\n",
        "- The proportion of female customers churning is greater than that of male customers\n",
        "- Interestingly, the majority of the customers that churned are those with the bank's credit card. This may be a coincidence, since the majority of the customers have the bank's credit card.\n",
        "- Unsurprisingly, the inactive members churn more. It is a concern that the overall proportion of inactive mebers is high. Perhaps the bank can implement a program to turn this group into active customers? This would definitely decrease customer churn.\n",
        "- Customers that have a life insurance plan with the bank tend to stick around. Perhaps this is an opportunity to create a customer retention measure?\n",
        "- Most customers do not have platinum status; those who do, don't churn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2A4P6TBw9Hz"
      },
      "source": [
        "## 4.2 We can also explore probability distributions of termination by variables such as age "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSvx5gfgw9Hz"
      },
      "outputs": [],
      "source": [
        "# 1. Visualize the distributions of numerical variables by status (i.e., our target variable)\n",
        "sns.set(font_scale = 1.5)\n",
        "for col in ['Age','FICOScore','Balance','Products','RegDeposits']:\n",
        "  facet = sns.FacetGrid(df, hue=\"Terminated\",aspect=4)\n",
        "  facet.map(sns.kdeplot, col, shade= True)\n",
        "  facet.set(xlim=(df[col].min(), df[col].max()))\n",
        "  facet.add_legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm3Mi1-ww9H0"
      },
      "source": [
        "**Alternatively, we can use box-plots!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLFuTccxw9H1"
      },
      "outputs": [],
      "source": [
        "# 2. Boxplots for numerical variables\n",
        "sns.set(font_scale = 1.25)\n",
        "fig, axarr = plt.subplots(2,3, figsize=(20, 12))\n",
        "sns.boxplot(y='Age',x = 'Terminated', hue = 'Terminated',data = df, ax=axarr[0][0])\n",
        "sns.boxplot(y='FICOScore',x = 'Terminated', hue = 'Terminated',data = df , ax=axarr[0][1])\n",
        "sns.boxplot(y='Balance',x = 'Terminated', hue = 'Terminated',data = df, ax=axarr[0][2])\n",
        "sns.boxplot(y='Products',x = 'Terminated', hue = 'Terminated',data = df, ax=axarr[1][0])\n",
        "sns.boxplot(y='RegDeposits',x = 'Terminated', hue = 'Terminated',data = df, ax=axarr[1][1])\n",
        "fig.tight_layout(pad=2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuoBxKmQw9H1"
      },
      "source": [
        "**We note the following:**\n",
        "\n",
        "- The older customers are churning more than the younger ones, suggesting differences in service preference across the age categories. The bank may need to review their target market or review the strategies for retention for different age groups\n",
        "- There is no significant difference in the credit score distribution between retained and churned customers.\n",
        "- The bank is losing customers with higher bank balances which is likely to hit their available capital for lending.\n",
        "- Neither the number of products nor the regular deposits has a significant effect on the likelihood to churn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "101sGR9Tw9H2"
      },
      "source": [
        "# 5. Feature Engineering\n",
        "\n",
        "Can we use the data to create some new features that may be predicitive of customers leaving the bank?\n",
        "In finance we often use ratios as meaningful KPIs (key performance indicators). Let's generate the Balance/Deposit ratio for the bank's customers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwjkkN1ow9H2"
      },
      "outputs": [],
      "source": [
        "# 1. Add a new variable for Balance to Deposit Ratio to our dataframe ('BalanceDepositRatio')\n",
        "df['BalanceDepositRatio'] = df.Balance/(df.RegDeposits+0.01)\n",
        "\n",
        "# 2. Visually inspect its relation to our target variable\n",
        "plt.figure(figsize=(10, 8), dpi=80)\n",
        "sns.boxplot(y='BalanceDepositRatio',x = 'Terminated', hue = 'Terminated',data = df)\n",
        "plt.ylim(-100, 100)\n",
        "sns.set(font_scale = 1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXWl54DeunAu"
      },
      "outputs": [],
      "source": [
        "print(f\"Median Balance-Deposit-Ratio Overall: {df.BalanceDepositRatio.median()}\")\n",
        "print(f\"Median Balance-Deposit-Ratio Retained: {df.BalanceDepositRatio[df['Terminated']==0].median()}\")\n",
        "print(f\"Median Balance-Deposit-Ratio Churned: {df.BalanceDepositRatio[df['Terminated']==1].median()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sf13vR8w9H3"
      },
      "source": [
        "- We saw that the salary has little effect on the chance of a customer churning. \n",
        "- However, the bank balance and the regular deposits indicate that customers with a higher balance deposit ratio churn more.\n",
        "- Our finding could be worrying to the bank because this impacts their source of loan capital"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcsvEH9Vw9H4"
      },
      "outputs": [],
      "source": [
        "# 3. Check that our data frame includes our new variable\n",
        "df.head()\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Kr4hxBw9H4"
      },
      "source": [
        "# 6. Data Preprocessing Pipeline\n",
        "Let's put it all together in a pipeline that we can use to:\n",
        "1. Test different models on the same data\n",
        "2. Apply to future bank data (e.g., validation data set)\n",
        "\n",
        "Let's re-load the data to start with a clean slate. \n",
        "  - This is also always the starting point for our analysis so that we can be sure that everything runs on the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYlYdDW06CBC"
      },
      "outputs": [],
      "source": [
        "# 1. Create a function that does all pre-processing steps for us\n",
        "\"\"\" Pre-processing Pipeline directly modifies dataframe df using .loc but then returns a new data frame with dummy variables.\n",
        "# drop_dummy defaults to True to drop one of each one hot encoded variables and avoid multicollinearity.\"\"\"\n",
        "\n",
        "def PrePipe(df, drop_dummy=True):\n",
        "    # 1a. Clean Categorical and Numeric data with our two Functions:\n",
        "    clean_BankChurn_categories(df)\n",
        "    clean_BankChurn_numeric(df)\n",
        "    \n",
        "    # 1b Feature Engineering\n",
        "    df['BalanceDepositRatio'] = df.Balance/(df.RegDeposits+0.0001)\n",
        "        \n",
        "    # 1c Reorder the columns, dropping unused\n",
        "    continuous_vars = ['FICOScore','Age','Balance','Products','RegDeposits', 'BalanceDepositRatio',]\n",
        "    cat_cols = ['Gender','Subsidiary']\n",
        "    zero_one_cols = ['BankCC','Active','LifeInsur', 'PlatStatus']\n",
        "\n",
        "    # 1d Mix-max scale the data between 0 and 1\n",
        "    df.loc[:,continuous_vars] = minmax_scale(df[continuous_vars])\n",
        "  \n",
        "    # 1e One-Hot Encode Categorical Variables\n",
        "    return pd.get_dummies(df[['Terminated'] + continuous_vars + zero_one_cols + cat_cols], columns = cat_cols, drop_first=drop_dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qawb1OQxw9H5"
      },
      "outputs": [],
      "source": [
        "# 2. Load data and Pre-process\n",
        "df = pd.read_json(\"Bank_Churn_Train.json\")\n",
        "df = PrePipe(df)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnvl_Nvdw9H6"
      },
      "source": [
        "# 7. Churn Prediction Model\n",
        "\n",
        "We are now ready to predict customer churn. At the top, we imported from scikit-learn\n",
        "- fit models\n",
        "- support functions\n",
        "- scoring functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXTA2S2F-Ycd"
      },
      "source": [
        "## 7.1 Classifier Performance\n",
        "We want to systematically evaluate the performance of our classifier. Here we define functions that output our performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTpzmJBRw9H7"
      },
      "outputs": [],
      "source": [
        "# 1. Function to output best model score, parameters and estimator\n",
        "def best_model(model):\n",
        "    print(model.best_score_)    \n",
        "    print(model.best_params_)\n",
        "    print(model.best_estimator_)\n",
        "\n",
        "# 2. Function to output accuracy score, visualize the confusion matrix, and print the classification report\n",
        "def show_results(y_test, y_pred):\n",
        "  # 2a. Output the accuracy of our prediction\n",
        "  print(accuracy_score(y_test, y_pred))\n",
        "  # 2b. Visualize the confusion matrix to make it easier to read\n",
        "  con_matrix = confusion_matrix(y_test, y_pred)\n",
        "  confusion_matrix_df = pd.DataFrame(con_matrix, ('Retained', 'Churned'), ('Retained', 'Churned'))\n",
        "  heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\", cmap=\"Blues\")\n",
        "  heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
        "  heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
        "  plt.ylabel('Actual', fontsize = 14)\n",
        "  plt.xlabel('Predicted', fontsize = 14)\n",
        "  # 2c. Print the classification report\n",
        "  print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em5OHkDmw9H8"
      },
      "source": [
        "## 7.2 Let's use Logistic Regression to predict customer churn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn7lXoDjw9H7"
      },
      "source": [
        "### 7.2.1 Load and Pre-process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UOni9idw9H7"
      },
      "outputs": [],
      "source": [
        "# 1. Start by loading and pre-processing our data\n",
        "df = pd.read_json(\"Bank_Churn_Train.json\")\n",
        "df = PrePipe(df, drop_dummy=True)\n",
        "\n",
        "# 2. Separate our target and input variables. \n",
        "y = df.Terminated\n",
        "X = df.drop(columns=['Terminated'])\n",
        "\n",
        "# 3. split sample into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "# 4. Check if data have right shape\n",
        "print(\"Train: Response Variable: \",y_train.shape)\n",
        "print(\"Train: Feature Variables: \",X_train.shape)\n",
        "print(\"Test: Response Variable: \",y_test.shape)\n",
        "print(\"Test: Feature Variables: \",X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N39-t-Luatdt"
      },
      "source": [
        "### 7.2.2 Train a Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAswIH2DfjPH"
      },
      "outputs": [],
      "source": [
        "# 1. Instantiate the classifier: logreg\n",
        "logreg = LogisticRegression(solver='lbfgs', C=1, tol=.0001, max_iter=1000)\n",
        "\n",
        "# 2. Fit the classifier to the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# 3. Predict the labels of the test set: y_pred\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# 4. Call function to evaluate model performance and show results\n",
        "show_results(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. We can examine the intercept as follows:\n",
        "print (f'Logistic Regression Intercept: {logreg.intercept_[0]}')\n",
        "\n",
        "# 6. We can examine the coefficients as follows:\n",
        "print (f'\\nLogistic Regression Coefficients: {logreg.coef_[0]}')"
      ],
      "metadata": {
        "id": "fp2bj1ly2QIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Which \"Square\" of the Confusion Matrix do we care for most?**\n",
        "\n",
        "**Do you remember what we learned in Class 05 ?**\n",
        "\n",
        "------------\n",
        "<p style=\"text-align: left; font-size:120%; font-weight: normal; font-style: normal;\">\n",
        "$\\text{Accuracy} = \\frac{t_p + t_n}{t_p + t_n + f_p + f_n}$ <br><br> \n",
        "$\\text{Precision} = \\frac{t_p}{t_p + f_p}$    <br><br>    \n",
        "$\\text{Recall} = \\frac{t_p}{t_p + f_n}$   <br><br>    \n",
        "$F_1 \\text{ score} = 2 \\times \\frac{\\textit{precision}\\, \\times \\,\\textit{recall}}{\\textit{precision}\\, + \\,\\textit{recall}}$ \n",
        "</p>\n",
        "\n",
        "------------\n",
        "\n",
        "\n",
        "####**Precision** measures the ability of the classifier not to mislabel a negative sample as positive\n",
        "####**Recall** measures the ability of the classifier to find all the positive samples."
      ],
      "metadata": {
        "id": "2eORO7rDN2tY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eVACuiEw9H8"
      },
      "source": [
        "### 7.2.3 Can we improve our results?\n",
        "\n",
        "Let's tune our hyperparameters using a Grid-Search with Cross-Fold validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90__U-Vfw9H8"
      },
      "outputs": [],
      "source": [
        "# 1. Define parameter space to test\n",
        "param_grid = {'C': [.1,1,10], 'tol':[.001, .0001, .00001]}\n",
        "\n",
        "# 2. Instantiate model: Note that we can define which score (i.e., performance metric such as precision or recall) we want to tune the hyperparameters towards. WHY would we do so?\n",
        "log_Grid = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000),param_grid, cv=5, refit=True, verbose=0, scoring = 'recall')\n",
        "\n",
        "# 3. Fit model to data\n",
        "log_Grid.fit(X_train, y_train);\n",
        "\n",
        "# 4. Show model accuracy and best parameters (i.e., tuned)\n",
        "print(best_model(log_Grid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Cl8VqiJw9H8"
      },
      "outputs": [],
      "source": [
        "# 5. Model is already trained with the optimal parameters identified and set\n",
        "y_pred = log_Grid.predict(X_test)\n",
        "\n",
        "# 6. Call function to evaluate model performance and show results\n",
        "show_results(y_test, y_pred)\n",
        "\n",
        "# The code below produces the same results because we set refit=True in our grid search above\n",
        "\n",
        "#LR=LogisticRegression(C=10, solver='lbfgs', tol=0.001)\n",
        "#LR.fit(X_train, y_train);\n",
        "#y_pred = LR.predict(X_test)\n",
        "#show_results(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKwhguntw9H9"
      },
      "source": [
        "## 7.3 Can a Random Forest Model do better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVMetGQZbSnZ"
      },
      "source": [
        "### 7.3.1 Load and Pre-process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmc5vTkubSxX"
      },
      "outputs": [],
      "source": [
        "# 1. Start by loading and pre-processing our data\n",
        "df = pd.read_json(\"Bank_Churn_Train.json\")\n",
        "df = PrePipe(df, drop_dummy=True)\n",
        "\n",
        "# 2. Separate our target and input variables. \n",
        "y = df.Terminated\n",
        "X = df.drop(columns=['Terminated'])\n",
        "\n",
        "# 3. split sample into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "# 4. Check if data have right shape\n",
        "print(\"Train: Response Variable: \",y_train.shape)\n",
        "print(\"Train: Feature Variables: \",X_train.shape)\n",
        "print(\"Test: Response Variable: \",y_test.shape)\n",
        "print(\"Test: Feature Variables: \",X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leyYZW9cbbww"
      },
      "source": [
        "### 7.3.2 Train a Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHJZJCDFw9H-"
      },
      "outputs": [],
      "source": [
        "# 1. Instantiate a Random Forest Classifier (RandomForestClassifier was previously imported from sklearn)\n",
        "forest = RandomForestClassifier(max_features=6, n_estimators=25, max_depth=9,random_state=21)\n",
        "\n",
        "# 2. Train the model using the training sets\n",
        "forest.fit(X_train, y_train)  \n",
        "\n",
        "# 3. Predict the response for test dataset\n",
        "y_pred = forest.predict(X_test)\n",
        "\n",
        "# 4. Call function to evaluate and show model performance\n",
        "show_results(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKQT3VAyIQ6j"
      },
      "source": [
        "### 7.3.3 Let's tune our hyperparameters using a Grid-Search with Cross-Fold validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMD9rLvJIQWE"
      },
      "outputs": [],
      "source": [
        "# 1. Define grid (i.e., hyperparameter combinations to test for)\n",
        "param_grid = {'n_estimators': [10, 25, 50], 'max_depth' : [6, 9, 12], 'max_features' : [3, 6, 9]}\n",
        "\n",
        "# 2. Instantiate the model (do not include parameters from the parameter grid in the classifier that you use; here, RandomForestClassifier())\n",
        "forest_Grid = GridSearchCV(RandomForestClassifier(random_state=21), param_grid, cv=5, refit=True, verbose=0, scoring = 'recall')\n",
        "\n",
        "# 3. Fit the model (i.e., train it on training data)\n",
        "forest_Grid.fit(X_train, y_train);\n",
        "\n",
        "# 4. Output optimal Hyperparameter combination\n",
        "print(best_model(forest_Grid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwnIETDfIQZ2"
      },
      "outputs": [],
      "source": [
        "# 5. Model is already trained with the optimal parameters identified and set: Use it to make prediction\n",
        "y_pred = forest_Grid.predict(X_test)\n",
        "\n",
        "# 6. Call function to evaluate model performance and show results\n",
        "show_results(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnz6h6mfw9H-"
      },
      "source": [
        "### 7.3.4 Let's take a look at which features are most important for our prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6tBk0YSw9H-"
      },
      "outputs": [],
      "source": [
        "# 1. Extract features and their importances\n",
        "feat_importances = pd.Series(forest.feature_importances_, index=X_train.columns)\n",
        "\n",
        "# 2. Sort importances_rf\n",
        "sorted_importances_rf = feat_importances.sort_values()\n",
        "\n",
        "# 3. Make a horizontal bar plot\n",
        "plt.figure(figsize=(12,8))\n",
        "sorted_importances_rf.plot(kind='barh', color='skyblue'); \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRr-pvyJsYDh"
      },
      "source": [
        "# 8. Making things better: Did we overlook something?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0J6Ugd2KFN9"
      },
      "outputs": [],
      "source": [
        "# 1. Start by loading our data\n",
        "df = pd.read_json(\"Bank_Churn_Train.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrXam9u0KFVV"
      },
      "outputs": [],
      "source": [
        "# 2. Let's take a close look at the variables again\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dkkDu7yj7gf"
      },
      "source": [
        "**Which variables did we previously drop?**\n",
        "- ClientID\n",
        "- Surname\n",
        "- Firstname"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12URWWHikqnz"
      },
      "source": [
        "## 8.1 Extract Information from a Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3St5Bxmbpsg0"
      },
      "outputs": [],
      "source": [
        "# 1. Extract CityCode from ClientID\n",
        "df['CityCode'] = df.ClientID.str[:5]\n",
        "\n",
        "# 2. Extract Year from ClientID\n",
        "df['Year'] = df.ClientID.str[5:9].astype('int')\n",
        "\n",
        "# 3. Keep only those after 2005 \n",
        "df = df[df.Year > 2005]\n",
        "\n",
        "# 4. Take a look\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NXfoZqhr9Gc"
      },
      "outputs": [],
      "source": [
        "# 5. It looks as if CityCode replicates Subsidiary\n",
        "df[['Subsidiary','CityCode']].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxn_sG9AlZGE"
      },
      "source": [
        "**We won't need all of these - let's drop the ones that we think will not help our model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-iklAEww9Hv"
      },
      "outputs": [],
      "source": [
        "# 6. Drop replicate columns and those that carry no meaning for our model\n",
        "df = df.drop([\"CityCode\"], axis = 1) # we used it just for exploration\n",
        "df = df.drop([\"ClientID\", \"Surname\", \"Firstname\"], axis = 1) \n",
        "\n",
        "# ... And check which ones are left\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAxxnqvrln5l"
      },
      "source": [
        "## 8.2 Feature Engineering from \"Year\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV1pNsPiw9H3"
      },
      "outputs": [],
      "source": [
        "# 1. Rather than looking at Year, let's look at the number of years the customer has been with the bank.\n",
        "df['Tenure'] = 2022 - df.Year\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wSI4K_kl_gn"
      },
      "outputs": [],
      "source": [
        "# 2. We may suspect that older people may have longer Tenure, so let's look at that ratio. \n",
        "df['TenureByAge'] = df.Tenure/(df.Age)\n",
        "\n",
        "# 2a. Let's look at the distributions\n",
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "sns.boxplot(y='TenureByAge',x = 'Terminated', hue = 'Terminated',data = df)\n",
        "sns.set(font_scale = 1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY7Cu60rmDOD"
      },
      "outputs": [],
      "source": [
        "# 3. Lastly we introduce a variable to capture credit score given age to take into account credit behavior visavis adult life\n",
        "df['FICOScoreGivenAge'] = df.Age/df.FICOScore*10\n",
        "\n",
        "# 3a. Let's look at the distributions\n",
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "sns.boxplot(y='FICOScoreGivenAge',x = 'Terminated', hue = 'Terminated',data = df)\n",
        "sns.set(font_scale = 1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jh58n6MnNl0"
      },
      "source": [
        "## 8.3 Update Pre-Processing Pipeline\n",
        "- Get Year from ClientID\n",
        "- Feature Engineer Tenure variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RadCHw_97gTa"
      },
      "outputs": [],
      "source": [
        "# 4. Update Pre-Processing Pipeline\n",
        "\n",
        "''' Updated function that does all of the pre-processing for us\n",
        "It directly modifies dataframe df using .loc but then returns a new data frame with dummy variables.\n",
        "optionally drop_dummy defaults to True to drop one of each one hot encoded variables and avoid multicollinearity.'''\n",
        "def PrePipe(df, drop_dummy=True):  \n",
        "    # 4a. Clean variables\n",
        "    clean_BankChurn_categories(df)\n",
        "    clean_BankChurn_numeric(df)\n",
        "    \n",
        "    # 4b. Engineer Features\n",
        "    df['BalanceDepositRatio'] = df.Balance/(df.RegDeposits+0.0001)\n",
        "\n",
        "    # 4c. Engineer NEW Tenure Features\n",
        "    df['Year'] = df.ClientID.str[5:9].astype('int')\n",
        "    df['Tenure'] = 2022 - df.Year\n",
        "    df['TenureByAge'] = df.Tenure/(df.Age)\n",
        "    df['FICOScoreGivenAge'] = (df.Age)/df.FICOScore*10\n",
        "     \n",
        "    # 4d. Reorder the columns, dropping unused. NEW: now includes our new tenure-based features\n",
        "    continuous_vars = ['FICOScore','Age','Balance','Products','RegDeposits', 'Tenure','BalanceDepositRatio','TenureByAge','FICOScoreGivenAge']\n",
        "    cat_cols = ['Gender','Subsidiary']\n",
        "    zero_one_cols = ['BankCC','Active','LifeInsur', 'PlatStatus']\n",
        "\n",
        "    # 4. Decision Trees do not require variables to be on the same scale; let's skip this step by commenting it out\n",
        "    # df.loc[:,continuous_vars] = minmax_scale(df[continuous_vars])\n",
        "  \n",
        "    # One-Hot Encode Categorical Variables\n",
        "    return pd.get_dummies(df[['Terminated'] + continuous_vars + zero_one_cols + cat_cols], columns = cat_cols, drop_first=drop_dummy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HphevgFcnflJ"
      },
      "source": [
        "## 8.4 Train Model again and Evaluate\n",
        "- Does Tenure matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3HoQczsodxb"
      },
      "source": [
        "### 8.4.1 Load and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER2PnfWCoFvO"
      },
      "outputs": [],
      "source": [
        "# 1. Start by loading and pre-processing our data\n",
        "df = pd.read_json(\"Bank_Churn_Train.json\")\n",
        "df = PrePipe(df, drop_dummy=True)\n",
        "\n",
        "# 2. Separate our target and input variables. \n",
        "y = df.Terminated\n",
        "X = df.drop(columns=['Terminated'])\n",
        "\n",
        "# 3. split sample into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "# 4. Check if data have right shape\n",
        "print(\"Train: Response Variable: \",y_train.shape)\n",
        "print(\"Train: Feature Variables: \",X_train.shape)\n",
        "print(\"Test: Response Variable: \",y_test.shape)\n",
        "print(\"Test: Feature Variables: \",X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp__-xDPognq"
      },
      "source": [
        "### 8.4.1 Random Forest Classifier\n",
        "- Hyperparameter tuning\n",
        "- Cross-Fold Validation\n",
        "- Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdgfg4NDomjh"
      },
      "outputs": [],
      "source": [
        "# 1. Define grid (i.e., hyperparameter combinations to test for)\n",
        "param_grid = {'n_estimators': [25, 50], 'max_depth' : [10, 20], 'max_features' : [5, 7]}\n",
        "\n",
        "# 2. Instantiate the model (do not include parameters from the parameter grid in the classifier that you use; here, RandomForestClassifier())\n",
        "NewForest_Grid = GridSearchCV(RandomForestClassifier(random_state=21), param_grid, cv=5, refit=True, n_jobs=-1, verbose=0, scoring = 'recall')\n",
        "\n",
        "# 3. Fit the model (i.e., train it on training data)\n",
        "NewForest_Grid.fit(X_train, y_train);\n",
        "\n",
        "# 4. Output optimal Hyperparameter combination\n",
        "print(best_model(NewForest_Grid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQp7uAtpSYI"
      },
      "source": [
        "***Use parameters to train and fit model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUJPTp0IpAWn"
      },
      "outputs": [],
      "source": [
        "# 5. Instantiate a Random Forest Classifier (RandomForestClassifier was previously imported from sklearn)\n",
        "NewForest = RandomForestClassifier(max_features=7, n_estimators=50, max_depth=20, random_state=21)\n",
        "\n",
        "# 6. Train the model using the training sets\n",
        "NewForest.fit(X_train, y_train)  \n",
        "\n",
        "# 7. Predict the response for test dataset\n",
        "y_pred = NewForest.predict(X_test)\n",
        "\n",
        "# 8. Call function to evaluate and show model performance\n",
        "show_results(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6ZmDL2GpuJM"
      },
      "source": [
        "***What about Feature Importance? Did anything change?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bYFJOAJpAlg"
      },
      "outputs": [],
      "source": [
        "# 9. Extract features and their importances\n",
        "feat_importances = pd.Series(NewForest.feature_importances_, index=X_train.columns)\n",
        "\n",
        "# 10. Sort importances_rf\n",
        "sorted_importances_rf = feat_importances.sort_values()\n",
        "\n",
        "# 11. Make a horizontal bar plot\n",
        "plt.figure(figsize=(12,8))\n",
        "sorted_importances_rf.plot(kind='barh', color='skyblue'); \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyJstssftZVi"
      },
      "source": [
        "# 9. Finalize our Model\n",
        "- We find that\n",
        "  - Random Forest beats Logistic Regression in our empirical setting. \n",
        "  - Hyperparameter tuning improves performance slightly\n",
        "  - Feature Engineering \"Tenure\" dramatically improves model performance\n",
        "- Let's train a ***Final Random Forest Model*** on all variables:\n",
        "    - Use entire training data\n",
        "    - Use Cross-Fold validation\n",
        "    - Hyperparameter tuning: Randomized Search CV for faster tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BEXOBlRtVsA"
      },
      "outputs": [],
      "source": [
        "# 1. Start by loading and pre-processing our data\n",
        "df = pd.read_json(\"Bank_Churn_Train.json\")\n",
        "df = PrePipe(df, drop_dummy=True)\n",
        "\n",
        "# 2. Separate our target and input variables. \n",
        "y = df.Terminated\n",
        "X = df.drop(columns=['Terminated'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An exhaustive search of the parameter grid can take a long time.** Particularly:\n",
        "- when there are many parameters to tune\n",
        "- when these parameters can assume many different values\n",
        "- on a slow computer (or limited virtual environment, like free CoLab)"
      ],
      "metadata": {
        "id": "A0F4dQHuM6-6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThUMTALcuo-v"
      },
      "outputs": [],
      "source": [
        "# 3. Define grid (i.e., hyperparameter combinations to test for)\n",
        "#param_grid = {'n_estimators': [25, 50], 'max_depth' : [10, 20], 'max_features' : [5, 7]}\n",
        "\n",
        "# 4. Instantiate the model (do not include parameters from the parameter grid in the classifier that you use; here, RandomForestClassifier())\n",
        "#FinalForest_Grid = GridSearchCV(RandomForestClassifier(random_state=21), param_grid, cv=5, refit=True, n_jobs=-1, verbose=0, scoring = 'recall')\n",
        "\n",
        "# 5. Fit the model (i.e., train it on training data)\n",
        "#FinalForest_Grid.fit(X, y);\n",
        "\n",
        "# 6. Output optimal Hyperparameter combination\n",
        "#print(best_model(FinalForest_Grid))\n",
        "\n",
        "# 7. Instantiate a Random Forest Classifier (RandomForestClassifier was previously imported from sklearn)\n",
        "#FinalForest = RandomForestClassifier(max_features=6, n_estimators=100, max_depth=12,random_state=21)\n",
        "\n",
        "# 8. Train the model using the training sets\n",
        "#FinalForest.fit(X, y) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***To speed things up***: Use RandomizedSearchCV. \n",
        "\n",
        "In contrast to `GridSearchCV`, not all parameter values are tried, but rather a fixed number of parameter settings is sampled from specified distributions. The number of parameter settings that are tried is given by `n_iter`."
      ],
      "metadata": {
        "id": "dQ6xeDK5NJce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df-wu6FEydAL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# 1 .define search\n",
        "param_grid = {'n_estimators': [25, 50, 100], 'max_depth' : [10, 15, 20], 'max_features' : [3, 5, 7]}\n",
        "search = RandomizedSearchCV(RandomForestClassifier(random_state=21), param_grid, n_iter=15, scoring='recall', n_jobs=-1, cv=5, random_state=21)\n",
        "\n",
        "# 5. Fit the model (i.e., train it on training data)\n",
        "search.fit(X, y);\n",
        "\n",
        "# 6. Output optimal Hyperparameter combination\n",
        "print(best_model(search))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CADHb4RK4Iu0"
      },
      "outputs": [],
      "source": [
        "# 7. Instantiate a Random Forest Classifier (RandomForestClassifier was previously imported from sklearn): Set the parameters that we found with RandomizedSearchCV\n",
        "FinalForest = RandomForestClassifier(max_depth=20, max_features=7, n_estimators=50, random_state=21)\n",
        "\n",
        "# 8. Train the model using the training sets\n",
        "FinalForest.fit(X, y)  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Predict with our final model\n",
        "y_pred = FinalForest.predict(X_test)\n",
        "\n",
        "# 10. Call function to evaluate and show model performance\n",
        "show_results(y_test, y_pred)"
      ],
      "metadata": {
        "id": "DiH9eHY5eP2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Save our Model in a file\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(FinalForest, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "abSKn4syYuhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNYujiR8b3HW"
      },
      "source": [
        "# 10. Use our Model to Decide on Who to Fight for and Who to Let Go\n",
        "- Use our model to predict churn\n",
        "- Determine who to let go and who to keep\n",
        "  - Need a metric for customer value!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load our trained Model from file\n",
        "filename = 'finalized_model.sav'\n",
        "FinalForest = pickle.load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "mcePmOPDZD6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQFI40fSK-Ei"
      },
      "outputs": [],
      "source": [
        "# 1. Load New Customer Data and take a first look\n",
        "NewCustomers = pd.read_json(\"Bank_Churn_NewCustomers.json\")\n",
        "NewCustomers.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NewCustomers.shape"
      ],
      "metadata": {
        "id": "WG4kQ8JHw1M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7ckM8dNMgUF"
      },
      "source": [
        "**How are these data different to our training data?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foZQX7VZMus3"
      },
      "source": [
        "## 10.1 Prepare New Customer Data for Prediction\n",
        "  - Cannot drop customers - Why?\n",
        "  - No response variable - Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NsEC5THVX5W"
      },
      "source": [
        "### 10.1.1 Update Preprocessing\n",
        "- Need to update preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNj5y-kxQpmT"
      },
      "outputs": [],
      "source": [
        "# 1. Update Function that handles the Cleaning of Categorical Variables: There is no variable \"Terminated\" in New Customer Data\n",
        "''' \n",
        "This function is to clean categories in the New Customer Bank Churn Data.\n",
        "Directly modifies the data frame df using .loc \n",
        "'''\n",
        "def clean_BankChurn_categories(df):\n",
        "  \n",
        "  # 1a. Identify the two types of categorical data \n",
        "  cat_cols = ['Gender','Subsidiary']\n",
        "  zero_one_cols = ['BankCC','Active','LifeInsur', 'PlatStatus']\n",
        "\n",
        "  # 1b. Clean-up Gender\n",
        "  df.loc[df.Gender.str.startswith('F'), 'Gender'] = 'Female'\n",
        "  df.loc[df.Gender.str.startswith('M'), 'Gender'] = 'Male'\n",
        "  \n",
        "  # 1c. Fix PlatStatus\n",
        "  if (df.PlatStatus.dtype == 'object') and (df.PlatStatus == 'yes').any():  # recode \"yes\" to 1, if present\n",
        "    df.loc[df.PlatStatus == 'yes', 'PlatStatus'] = '1'\n",
        "  if (df.PlatStatus.dtype == 'object') and (df.PlatStatus == 'no').any():   # recode \"no\" to 0, if present\n",
        "    df.loc[df.PlatStatus == 'yes', 'PlatStatus'] = '0'\n",
        "  df.loc[:,'PlatStatus'] = df['PlatStatus'].astype(int)\n",
        "\n",
        "  # 1d. Enforce boundaries for zero/one columns:\n",
        "  for col in zero_one_cols:\n",
        "    df.loc[:,col].clip(0,1, inplace = True) # \"clip\" assigns values outside boundary to boundary values.\n",
        "\n",
        "  # 1e. Typecast all categorical and zero/one columns to categorical\n",
        "  for col in cat_cols+zero_one_cols:\n",
        "    df.loc[:,col] = df[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okUbnR2CQ87f"
      },
      "outputs": [],
      "source": [
        "# 2. Update Function that Cleans the Numerical Variables: Cannot drop customers! Need to predict them all.\n",
        "''' \n",
        "This function is to clean numeric fields of the New Customer Bank Churn data.\n",
        "Directly modifies the data frame df using .loc and drop inplace.\n",
        "'''\n",
        "def clean_BankChurn_numeric(df):\n",
        "  \n",
        "  # 2a. Impute invalid data with medians\n",
        "  df.loc[df.Age > 100,'Age'] = df.Age.median()\n",
        "  df.loc[df.Products > 10, 'Products'] = df.Products.median()\n",
        "  \n",
        "  # 2b. Set values outside of valid ranges to valid values (at limits)\n",
        "  df.loc[df.FICOScore<=299, 'FICOScore'] = 300\n",
        "  df.loc[df.Balance < -5000, 'Balance'] = -5000\n",
        "  df.loc[df.Balance > 5e5, 'Balance'] = 5e5\n",
        "  df.loc[df.RegDeposits < 0, 'RegDeposits'] = 0\n",
        "  df.loc[df.RegDeposits > 1e5, 'RegDeposits'] = 1e5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QsjjKMrTz8w"
      },
      "outputs": [],
      "source": [
        "# 3. Update Preprocessing Function\n",
        "# It directly modifies dataframe df using .loc but then returns a new data frame with dummy variables.\n",
        "# optinonally drop_dummy defaults to True to drop one of each one hot encoded variables and avoid multicollinearity.\n",
        "def PrePipe(df, drop_dummy=True):\n",
        "    clean_BankChurn_categories(df)\n",
        "    clean_BankChurn_numeric(df)\n",
        "    \n",
        "    # Engineer Features\n",
        "    df['BalanceDepositRatio'] = df.Balance/(df.RegDeposits+0.0001)\n",
        "\n",
        "    # Engineer NEW Tenure Features\n",
        "    df['Year'] = df.ClientID.str[5:9].astype('int')\n",
        "    df['Tenure'] = 2022 - df.Year\n",
        "    df['TenureByAge'] = df.Tenure/(df.Age)\n",
        "    df['FICOScoreGivenAge'] = (df.Age)/df.FICOScore*10\n",
        "     \n",
        "    # Reorder the columns, dropping unused. NEW: now includes our new tenure-based features\n",
        "    continuous_vars = ['FICOScore','Age','Balance','Products','RegDeposits', 'Tenure','BalanceDepositRatio','TenureByAge','FICOScoreGivenAge']\n",
        "    cat_cols = ['Gender','Subsidiary']\n",
        "    zero_one_cols = ['BankCC','Active','LifeInsur', 'PlatStatus']\n",
        "\n",
        "    # mix-max scale the data between 0 and 1: won't use because we don't need to with a tree model\n",
        "    #df.loc[:,continuous_vars] = minmax_scale(df[continuous_vars])\n",
        "  \n",
        "    # One-Hot Encode Categorical Variables\n",
        "    return pd.get_dummies(df[continuous_vars + zero_one_cols + cat_cols], columns = cat_cols, drop_first=drop_dummy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvBY0R16Uohe"
      },
      "source": [
        "### 10.1.2 Remove Variables that Model was not Trained on\n",
        "-  Our model has not seen Bank Revenue in its training. Must remove it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JEMz1ubdP0K"
      },
      "outputs": [],
      "source": [
        "# 1. Create new Dataframe that includes Variables that our Model was trained on. VERY IMPORTANT! Why?\n",
        "# 1a Drop Bank Revenue\n",
        "df = NewCustomers.drop(columns=['BnkRev'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbnLkkcvVkKw"
      },
      "source": [
        "## 10.1.3 Preprocess Data with New Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfU2-FWYNTXH"
      },
      "outputs": [],
      "source": [
        "# 2. Preprocess data with our Preprocessing Pipeline\n",
        "X = PrePipe(df, drop_dummy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQpWF15IO5Yq"
      },
      "source": [
        "## 10.2 Use our Trained Model to predict which Customers will Churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgAoYg9FPCUD"
      },
      "outputs": [],
      "source": [
        "# 1. Predict with Base Random Forest Model\n",
        "y_pred = FinalForest.predict(X)\n",
        "\n",
        "# 2. Add prediction to New Customer Data\n",
        "NewCustomers['AtRisk']=y_pred\n",
        "\n",
        "# 3. Check if it worked\n",
        "NewCustomers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBGTlb2ZWPmM"
      },
      "source": [
        "## 10.3 Quantify Financial Risk to Bank\n",
        "- How much Revenue is at Stake?\n",
        "  - Depends of Customer Revenue\n",
        "  - Depends on Probability that customer churns\n",
        "- Informs how much to spend on customer retention measures!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb5jHkhsX2XL"
      },
      "source": [
        "### 10.3.1 Use Model to estimate Churn Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4QvAR9SPEZF"
      },
      "outputs": [],
      "source": [
        "# 1. Predict churn probabilities and add directly to New Customer Data\n",
        "NewCustomers['ChurnProb']=FinalForest.predict_proba(X)[:, 1]\n",
        "\n",
        "# 2. Check if it worked\n",
        "NewCustomers.sort_values(by=['ChurnProb'], ascending=False, inplace=True)\n",
        "NewCustomers.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NewCustomers.tail(10)"
      ],
      "metadata": {
        "id": "1Hfqgs35yREE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Predict Churn Probability of an individual customer\n",
        "\n",
        "# 3a. Describe Customer along the variable that our model was trained on\n",
        "FICOScore=788\n",
        "Age=22\n",
        "Balance=12000\n",
        "Products=1\n",
        "RegDeposits=6000\n",
        "Tenure=2\n",
        "BalanceDepositRatio=Balance/(RegDeposits+0.0001)\n",
        "TenureByAge=Tenure/Age\n",
        "FICOScoreGivenAge=Age/FICOScore*10\n",
        "BankCC=1\n",
        "Active=1\n",
        "LifeInsur=0\n",
        "PlatStatus=0\n",
        "Gender_Male=1\n",
        "Subsidiary_Boston=0\n",
        "Subsidiary_Chapel_Hill=1\n",
        "\n",
        "# 3b. Construct a DataFrame for our individual customer that we can pass to our model\n",
        "x_new = pd.DataFrame([[FICOScore,Age,Balance,Products,RegDeposits,Tenure,BalanceDepositRatio,TenureByAge,FICOScoreGivenAge,BankCC,Active,LifeInsur,PlatStatus,Gender_Male,Subsidiary_Boston,Subsidiary_Chapel_Hill]],\n",
        "                      columns=['FICOScore','Age','Balance','Products','RegDeposits','Tenure','BalanceDepositRatio','TenureByAge','FICOScoreGivenAge','BankCC','Active','LifeInsur','PlatStatus','Gender_Male','Subsidiary_Boston','Subsidiary_Chapel Hill'])\n",
        "\n",
        "# 3c. Use our trained model to predict the probability that the customer will churn\n",
        "Cprop=FinalForest.predict_proba(x_new)[:, 1]\n",
        "print(f\"Predicted Probability to Churn: {Cprop}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ytB4twE3gIuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqcictX7X-XZ"
      },
      "source": [
        "### 10.3.2 Describe Churn Risk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL4-S7beYKQ4"
      },
      "outputs": [],
      "source": [
        "# 1. Describe Churn Probabilities\n",
        "NewCustomers.ChurnProb.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Visualize the distribution of Churn Probabilities \n",
        "sns.set(font_scale = 1.5)\n",
        "sns.displot(NewCustomers, x=\"ChurnProb\", height=6, aspect=1.5)"
      ],
      "metadata": {
        "id": "5aIXT02SUJQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyXIKhPSYVJZ"
      },
      "source": [
        "### 10.3.3 Investigate Bank Revenue at Risk\n",
        "One possibility: For each customer calculate `ChurnProb` $\\times$ `BnkRev`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKLDlIFrY2Ad"
      },
      "outputs": [],
      "source": [
        "# 1. Calculate new Variable RevAtRisk\n",
        "NewCustomers['RevAtRisk']=NewCustomers['ChurnProb'] * NewCustomers['BnkRev']\n",
        "\n",
        "# 2. Describe Revenue at Risk\n",
        "NewCustomers.RevAtRisk.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What is the total revenue at risk?\n",
        "NewCustomers.RevAtRisk.sum()"
      ],
      "metadata": {
        "id": "tSHJhGVpfofB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Visualize the distribution of Churn Probabilities \n",
        "sns.set(font_scale = 1.5)\n",
        "sns.displot(NewCustomers, x=\"RevAtRisk\", bins=6, height=6, aspect=1.5)"
      ],
      "metadata": {
        "id": "r99B6hB4V5Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACV4V0KLZZJn"
      },
      "outputs": [],
      "source": [
        "# 5. Look up the top 10 customers with highest revenue at risk\n",
        "NewCustomers.sort_values(by=['RevAtRisk'], ascending=False, inplace=True)\n",
        "NewCustomers.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.4 Where to from here?\n",
        "\n",
        "1. **What managerial decisions might our Model inform?**\n",
        "- \n",
        "- \n",
        "- \n",
        "\n",
        "2. **Are there other Variables to consider?**\n",
        "- \n",
        "- \n",
        "- \n",
        "\n",
        "3. **What are possible Limitations?**\n",
        "- \n",
        "- \n",
        "- "
      ],
      "metadata": {
        "id": "vHiEfL2vWNOg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAaOWGaYw9H-"
      },
      "source": [
        "# 11. How well did we do?\n",
        "Six months later, we know which customers churned (assuming that the bank did not implement any retention measures). \n",
        "\n",
        "***Let's go back and see how well our model predicted the churn of the new customers.***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load outcome data and take a look\n",
        "outcomes = pd.read_json(\"Bank_Churn_NewCustomers_Outcome.json\")\n",
        "outcomes.head()"
      ],
      "metadata": {
        "id": "aaxOeJMJSR7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Get outcome variable\n",
        "y = outcomes.Terminated"
      ],
      "metadata": {
        "id": "q6wpvIBLSi6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErYWzIsJSlzw"
      },
      "outputs": [],
      "source": [
        "# 3. Evaluate our Model's predictions from 6 months ago\n",
        "show_results(y, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.1 Where our Model failed to predict Churn\n"
      ],
      "metadata": {
        "id": "CUdUo1CaXaGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Sort our Data by its index to ensure that rows for prediction and truth are aligned\n",
        "NewCustomers.sort_index(inplace=True)\n",
        "\n",
        "# 2. Add Outcome to our NewCustomers dataframe\n",
        "NewCustomers['Terminated']=y\n",
        "\n",
        "# 3. Add variable for PredFailChurn (predicted not at risk, but ultimately churned)\n",
        "NewCustomers['PredFailChurn']=(NewCustomers['Terminated']==1) & (NewCustomers['AtRisk']==0)\n",
        "\n",
        "# 4. See where our Model fails:\n",
        "NewCustomers[NewCustomers['PredFailChurn']==True].head(25)"
      ],
      "metadata": {
        "id": "6RbSOtZ4bUVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.2 Impact of Model Failure\n",
        "\n",
        "Up to how much revenue might the bank lose because our model failed to identify customers that are at risk of churning?"
      ],
      "metadata": {
        "id": "U-guLjWvda0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Sum BnkRev for Customers where we failed to predict that they will churn\n",
        "print(f\"Lost revenue of churned customers that we failed to identify: $ {NewCustomers[NewCustomers['PredFailChurn']==True]['BnkRev'].sum()}\")"
      ],
      "metadata": {
        "id": "cpD4tDjsXfP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrSaMpgdwhJt"
      },
      "source": [
        "# 12. What Next?\n",
        "1. How to fix Model failure?  \n",
        "\n",
        "2. Predictions for new Customers?  \n",
        "\n",
        "3. Update Predictions?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce2qK-3RwhRl"
      },
      "source": [
        "# **Looking Ahead:**  \n",
        "\n",
        "####**Next Class:** Thrsday, March 23, 2023\n",
        "\n",
        "#### ***Algorithmic Bias*** \n",
        "\n",
        "#### **Read before class:** Read Lambrecht, A. and Tucker, C., 2019. [Algorithmic bias? An empirical study of apparent gender-based discrimination in the display of STEM career ads.](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2018.3093) Management Science, 65(7), pp.2966-2981.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Call for Nominations: Recognize a Professor for their Teaching** \n",
        "\n",
        "> **Put your Nomintations in before Monday, March 27th, 2023:** https://tinyurl.com/weatherspoon2023 \n",
        "\n",
        "![Weatherspoon](https://mapxp.app/BUSI488/Weatherspoon2023.png)\n"
      ],
      "metadata": {
        "id": "CmsVSFb7Wkp_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYz7VOVhweu0"
      },
      "source": [
        "This notebook was inspired by the following:  \n",
        "https://github.com/soanems/bank-customer-churn-python/blob/master/Bank%20Customer%20Churn_2.ipynb  \n",
        "https://www.kaggle.com/kmalit/bank-customer-churn-prediction  \n",
        "https://academy.vertabelo.com/blog/python-customer-churn-prediction/  \n",
        "http://dataskunkworks.com/2018/06/05/predicting-customer-churn-with-python-logistic-regression-decision-trees-and-random-forests/  \n",
        "https://www.neuraldesigner.com/learning/examples/bank-churn  \n",
        "https://www.kaggle.com/nasirislamsujan/bank-customer-churn-prediction?scriptVersionId=5729160  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}